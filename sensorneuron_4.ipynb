{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643fc36f-6ce0-4e73-83d5-62b89f4e4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SensorNeuron(nn.Module):\n",
    "    def __init__(self, input_size, dataset_samples, threshold=0.5):\n",
    "        super(SensorNeuron, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.threshold = threshold  # Limiar para sinal forte\n",
    "        self.sensory_db = self.initialize_sensory_neurons()  # Banco de dados sensorial\n",
    "        self.consolidation_counter = [0] * input_size  # Contadores de consolidação\n",
    "    \n",
    "    def initialize_sensory_neurons(self):\n",
    "        sensory_neurons = []\n",
    "        for _ in range(self.input_size):\n",
    "            random_sample = random.choice(self.dataset_samples)\n",
    "            sensory_neurons.append(random.choice(random_sample))  # Neurônios inicializados com dados aleatórios\n",
    "        return torch.tensor(sensory_neurons).float()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float()  # Garantir tipo correto de entrada\n",
    "        reinforcement_score = 0  # Inicia o score de reforço\n",
    "        \n",
    "        # Ordenar os índices dos neurônios de acordo com a consolidação (priorizando os reforçados)\n",
    "        sorted_indices = sorted(range(self.input_size), key=lambda i: self.consolidation_counter[i], reverse=True)\n",
    "        \n",
    "        for i in sorted_indices:  # Processa os neurônios reforçados primeiro\n",
    "            # Calcula a diferença absoluta\n",
    "            diff = torch.abs(x[:, i] - self.sensory_db[i])\n",
    "            \n",
    "            # Verifica se cada elemento no lote é menor que o threshold\n",
    "            signal_strong = diff < self.threshold\n",
    "            \n",
    "            # Verifica se a maioria dos sinais é forte (True)\n",
    "            if signal_strong.all():  # Se todos os valores no lote são menores que o limiar\n",
    "                self.consolidation_counter[i] += 1\n",
    "                reinforcement_score += 1  # Sinal forte encontrado, reforça a informação\n",
    "            else:\n",
    "                self.consolidation_counter[i] -= 1  # Sinal fraco, reduz a consolidação\n",
    "            \n",
    "            # Caso o contador de consolidação seja negativo, atualiza o neurônio\n",
    "            if self.consolidation_counter[i] < 0:\n",
    "                self.sensory_db = self.update_sensory_neuron(self.sensory_db, self.dataset_samples, i)  # Chama a função externa\n",
    "        \n",
    "        return x, reinforcement_score\n",
    "    \n",
    "    def update_sensory_neuron(self, sensory_db, dataset_samples, index):\n",
    "        random_sample = random.choice(dataset_samples)\n",
    "        new_value = random.choice(random_sample)\n",
    "        sensory_db[index] = new_value\n",
    "        return sensory_db\n",
    "        \n",
    "    def reinforce_database(self):\n",
    "        # Caso o sinal seja encontrado mais de uma vez, a informação é consolidada no banco de dados permanente\n",
    "        for i in range(self.input_size):\n",
    "            if self.consolidation_counter[i] > 5:  # Exemplo de limiar de consolidação\n",
    "                self.sensory_db[i] = self.sensory_db[i]  # Reforço da informação\n",
    "            elif self.consolidation_counter[i] < -3:\n",
    "                self.update_sensory_neuron(i)  # Atualiza os neurônios com base no contador de consolidação\n",
    "\n",
    "\n",
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dataset_samples, threshold=0.5):\n",
    "        super(AdvancedMLP, self).__init__()\n",
    "        self.sensor_neuron = SensorNeuron(input_size=input_size, dataset_samples=dataset_samples, threshold=threshold)  # Neurônios sensoriais\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, reinforcement_score = self.sensor_neuron(x)  # Processamento inicial com neurônios sensoriais\n",
    "        x = F.relu(self.fc1(x))  # Primeira camada de rede neural\n",
    "        x = self.fc2(x)  # Camada final de saída\n",
    "        return F.log_softmax(x, dim=1), reinforcement_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efcba44-a2c1-4b4a-88ad-bba7fb527245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.1869, Accuracy: 23.33%\n",
      "Epoch [2/200], Loss: 1.1660, Accuracy: 23.33%\n",
      "Epoch [3/200], Loss: 1.1474, Accuracy: 23.33%\n",
      "Epoch [4/200], Loss: 1.1278, Accuracy: 23.33%\n",
      "Epoch [5/200], Loss: 1.1114, Accuracy: 23.33%\n",
      "Epoch [6/200], Loss: 1.0921, Accuracy: 23.33%\n",
      "Epoch [7/200], Loss: 1.0749, Accuracy: 31.67%\n",
      "Epoch [8/200], Loss: 1.0561, Accuracy: 40.83%\n",
      "Epoch [9/200], Loss: 1.0386, Accuracy: 44.17%\n",
      "Epoch [10/200], Loss: 1.0198, Accuracy: 48.33%\n",
      "Epoch [11/200], Loss: 0.9998, Accuracy: 50.83%\n",
      "Epoch [12/200], Loss: 0.9817, Accuracy: 51.67%\n",
      "Epoch [13/200], Loss: 0.9633, Accuracy: 53.33%\n",
      "Epoch [14/200], Loss: 0.9449, Accuracy: 52.50%\n",
      "Epoch [15/200], Loss: 0.9273, Accuracy: 52.50%\n",
      "Epoch [16/200], Loss: 0.9054, Accuracy: 59.17%\n",
      "Epoch [17/200], Loss: 0.8899, Accuracy: 64.17%\n",
      "Epoch [18/200], Loss: 0.8678, Accuracy: 70.83%\n",
      "Epoch [19/200], Loss: 0.8526, Accuracy: 74.17%\n",
      "Epoch [20/200], Loss: 0.8348, Accuracy: 75.83%\n",
      "Epoch [21/200], Loss: 0.8226, Accuracy: 75.83%\n",
      "Epoch [22/200], Loss: 0.8017, Accuracy: 75.83%\n",
      "Epoch [23/200], Loss: 0.7888, Accuracy: 75.83%\n",
      "Epoch [24/200], Loss: 0.7682, Accuracy: 75.83%\n",
      "Epoch [25/200], Loss: 0.7581, Accuracy: 75.83%\n",
      "Epoch [26/200], Loss: 0.7416, Accuracy: 75.83%\n",
      "Epoch [27/200], Loss: 0.7262, Accuracy: 75.83%\n",
      "Epoch [28/200], Loss: 0.7116, Accuracy: 75.83%\n",
      "Epoch [29/200], Loss: 0.7040, Accuracy: 75.83%\n",
      "Epoch [30/200], Loss: 0.6897, Accuracy: 75.83%\n",
      "Epoch [31/200], Loss: 0.6833, Accuracy: 75.83%\n",
      "Epoch [32/200], Loss: 0.6641, Accuracy: 76.67%\n",
      "Epoch [33/200], Loss: 0.6640, Accuracy: 76.67%\n",
      "Epoch [34/200], Loss: 0.6535, Accuracy: 76.67%\n",
      "Epoch [35/200], Loss: 0.6437, Accuracy: 76.67%\n",
      "Epoch [36/200], Loss: 0.6263, Accuracy: 76.67%\n",
      "Epoch [37/200], Loss: 0.6289, Accuracy: 76.67%\n",
      "Epoch [38/200], Loss: 0.6081, Accuracy: 76.67%\n",
      "Epoch [39/200], Loss: 0.6060, Accuracy: 76.67%\n",
      "Epoch [40/200], Loss: 0.5998, Accuracy: 76.67%\n",
      "Epoch [41/200], Loss: 0.5945, Accuracy: 76.67%\n",
      "Epoch [42/200], Loss: 0.5865, Accuracy: 76.67%\n",
      "Epoch [43/200], Loss: 0.5732, Accuracy: 76.67%\n",
      "Epoch [44/200], Loss: 0.5619, Accuracy: 77.50%\n",
      "Epoch [45/200], Loss: 0.5640, Accuracy: 77.50%\n",
      "Epoch [46/200], Loss: 0.5565, Accuracy: 76.67%\n",
      "Epoch [47/200], Loss: 0.5512, Accuracy: 76.67%\n",
      "Epoch [48/200], Loss: 0.5589, Accuracy: 76.67%\n",
      "Epoch [49/200], Loss: 0.5390, Accuracy: 76.67%\n",
      "Epoch [50/200], Loss: 0.5386, Accuracy: 77.50%\n",
      "Epoch [51/200], Loss: 0.5287, Accuracy: 77.50%\n",
      "Epoch [52/200], Loss: 0.5345, Accuracy: 77.50%\n",
      "Epoch [53/200], Loss: 0.5207, Accuracy: 77.50%\n",
      "Epoch [54/200], Loss: 0.5159, Accuracy: 77.50%\n",
      "Epoch [55/200], Loss: 0.5142, Accuracy: 77.50%\n",
      "Epoch [56/200], Loss: 0.5091, Accuracy: 77.50%\n",
      "Epoch [57/200], Loss: 0.5034, Accuracy: 77.50%\n",
      "Epoch [58/200], Loss: 0.5002, Accuracy: 77.50%\n",
      "Epoch [59/200], Loss: 0.5056, Accuracy: 77.50%\n",
      "Epoch [60/200], Loss: 0.4914, Accuracy: 77.50%\n",
      "Epoch [61/200], Loss: 0.4961, Accuracy: 77.50%\n",
      "Epoch [62/200], Loss: 0.4785, Accuracy: 78.33%\n",
      "Epoch [63/200], Loss: 0.4815, Accuracy: 78.33%\n",
      "Epoch [64/200], Loss: 0.4780, Accuracy: 78.33%\n",
      "Epoch [65/200], Loss: 0.4678, Accuracy: 78.33%\n",
      "Epoch [66/200], Loss: 0.4645, Accuracy: 78.33%\n",
      "Epoch [67/200], Loss: 0.4634, Accuracy: 78.33%\n",
      "Epoch [68/200], Loss: 0.4639, Accuracy: 78.33%\n",
      "Epoch [69/200], Loss: 0.4540, Accuracy: 78.33%\n",
      "Epoch [70/200], Loss: 0.4498, Accuracy: 78.33%\n",
      "Epoch [71/200], Loss: 0.4470, Accuracy: 78.33%\n",
      "Epoch [72/200], Loss: 0.4475, Accuracy: 78.33%\n",
      "Epoch [73/200], Loss: 0.4411, Accuracy: 78.33%\n",
      "Epoch [74/200], Loss: 0.4389, Accuracy: 78.33%\n",
      "Epoch [75/200], Loss: 0.4397, Accuracy: 78.33%\n",
      "Epoch [76/200], Loss: 0.4361, Accuracy: 78.33%\n",
      "Epoch [77/200], Loss: 0.4328, Accuracy: 79.17%\n",
      "Epoch [78/200], Loss: 0.4273, Accuracy: 80.00%\n",
      "Epoch [79/200], Loss: 0.4283, Accuracy: 80.83%\n",
      "Epoch [80/200], Loss: 0.4186, Accuracy: 81.67%\n",
      "Epoch [81/200], Loss: 0.4209, Accuracy: 81.67%\n",
      "Epoch [82/200], Loss: 0.4141, Accuracy: 81.67%\n",
      "Epoch [83/200], Loss: 0.4102, Accuracy: 81.67%\n",
      "Epoch [84/200], Loss: 0.4086, Accuracy: 81.67%\n",
      "Epoch [85/200], Loss: 0.4005, Accuracy: 81.67%\n",
      "Epoch [86/200], Loss: 0.4037, Accuracy: 81.67%\n",
      "Epoch [87/200], Loss: 0.4064, Accuracy: 82.50%\n",
      "Epoch [88/200], Loss: 0.4042, Accuracy: 82.50%\n",
      "Epoch [89/200], Loss: 0.3996, Accuracy: 82.50%\n",
      "Epoch [90/200], Loss: 0.3975, Accuracy: 82.50%\n",
      "Epoch [91/200], Loss: 0.3950, Accuracy: 83.33%\n",
      "Epoch [92/200], Loss: 0.3958, Accuracy: 83.33%\n",
      "Epoch [93/200], Loss: 0.3958, Accuracy: 83.33%\n",
      "Epoch [94/200], Loss: 0.3919, Accuracy: 83.33%\n",
      "Epoch [95/200], Loss: 0.3861, Accuracy: 83.33%\n",
      "Epoch [96/200], Loss: 0.3866, Accuracy: 83.33%\n",
      "Epoch [97/200], Loss: 0.3872, Accuracy: 83.33%\n",
      "Epoch [98/200], Loss: 0.3783, Accuracy: 83.33%\n",
      "Epoch [99/200], Loss: 0.3779, Accuracy: 83.33%\n",
      "Epoch [100/200], Loss: 0.3777, Accuracy: 83.33%\n",
      "Epoch [101/200], Loss: 0.3758, Accuracy: 83.33%\n",
      "Epoch [102/200], Loss: 0.3722, Accuracy: 83.33%\n",
      "Epoch [103/200], Loss: 0.3714, Accuracy: 83.33%\n",
      "Epoch [104/200], Loss: 0.3756, Accuracy: 83.33%\n",
      "Epoch [105/200], Loss: 0.3675, Accuracy: 83.33%\n",
      "Epoch [106/200], Loss: 0.3649, Accuracy: 83.33%\n",
      "Epoch [107/200], Loss: 0.3679, Accuracy: 83.33%\n",
      "Epoch [108/200], Loss: 0.3624, Accuracy: 83.33%\n",
      "Epoch [109/200], Loss: 0.3661, Accuracy: 83.33%\n",
      "Epoch [110/200], Loss: 0.3616, Accuracy: 83.33%\n",
      "Epoch [111/200], Loss: 0.3605, Accuracy: 83.33%\n",
      "Epoch [112/200], Loss: 0.3610, Accuracy: 83.33%\n",
      "Epoch [113/200], Loss: 0.3536, Accuracy: 83.33%\n",
      "Epoch [114/200], Loss: 0.3534, Accuracy: 83.33%\n",
      "Epoch [115/200], Loss: 0.3542, Accuracy: 83.33%\n",
      "Epoch [116/200], Loss: 0.3505, Accuracy: 83.33%\n",
      "Epoch [117/200], Loss: 0.3519, Accuracy: 83.33%\n",
      "Epoch [118/200], Loss: 0.3477, Accuracy: 83.33%\n",
      "Epoch [119/200], Loss: 0.3452, Accuracy: 83.33%\n",
      "Epoch [120/200], Loss: 0.3414, Accuracy: 83.33%\n",
      "Epoch [121/200], Loss: 0.3506, Accuracy: 83.33%\n",
      "Epoch [122/200], Loss: 0.3415, Accuracy: 83.33%\n",
      "Epoch [123/200], Loss: 0.3459, Accuracy: 83.33%\n",
      "Epoch [124/200], Loss: 0.3430, Accuracy: 83.33%\n",
      "Epoch [125/200], Loss: 0.3465, Accuracy: 83.33%\n",
      "Epoch [126/200], Loss: 0.3378, Accuracy: 83.33%\n",
      "Epoch [127/200], Loss: 0.3390, Accuracy: 83.33%\n",
      "Epoch [128/200], Loss: 0.3436, Accuracy: 84.17%\n",
      "Epoch [129/200], Loss: 0.3379, Accuracy: 83.33%\n",
      "Epoch [130/200], Loss: 0.3347, Accuracy: 83.33%\n",
      "Epoch [131/200], Loss: 0.3309, Accuracy: 84.17%\n",
      "Epoch [132/200], Loss: 0.3318, Accuracy: 84.17%\n",
      "Epoch [133/200], Loss: 0.3242, Accuracy: 84.17%\n",
      "Epoch [134/200], Loss: 0.3232, Accuracy: 84.17%\n",
      "Epoch [135/200], Loss: 0.3219, Accuracy: 84.17%\n",
      "Epoch [136/200], Loss: 0.3225, Accuracy: 84.17%\n",
      "Epoch [137/200], Loss: 0.3241, Accuracy: 84.17%\n",
      "Epoch [138/200], Loss: 0.3210, Accuracy: 84.17%\n",
      "Epoch [139/200], Loss: 0.3236, Accuracy: 84.17%\n",
      "Epoch [140/200], Loss: 0.3180, Accuracy: 84.17%\n",
      "Epoch [141/200], Loss: 0.3269, Accuracy: 84.17%\n",
      "Epoch [142/200], Loss: 0.3122, Accuracy: 84.17%\n",
      "Epoch [143/200], Loss: 0.3121, Accuracy: 84.17%\n",
      "Epoch [144/200], Loss: 0.3259, Accuracy: 84.17%\n",
      "Epoch [145/200], Loss: 0.3179, Accuracy: 85.00%\n",
      "Epoch [146/200], Loss: 0.3091, Accuracy: 85.00%\n",
      "Epoch [147/200], Loss: 0.3175, Accuracy: 85.00%\n",
      "Epoch [148/200], Loss: 0.3076, Accuracy: 85.00%\n",
      "Epoch [149/200], Loss: 0.3044, Accuracy: 85.00%\n",
      "Epoch [150/200], Loss: 0.2998, Accuracy: 85.00%\n",
      "Epoch [151/200], Loss: 0.3102, Accuracy: 85.00%\n",
      "Epoch [152/200], Loss: 0.3032, Accuracy: 85.83%\n",
      "Epoch [153/200], Loss: 0.3053, Accuracy: 85.83%\n",
      "Epoch [154/200], Loss: 0.2992, Accuracy: 85.83%\n",
      "Epoch [155/200], Loss: 0.2987, Accuracy: 85.83%\n",
      "Epoch [156/200], Loss: 0.3029, Accuracy: 85.83%\n",
      "Epoch [157/200], Loss: 0.2949, Accuracy: 85.83%\n",
      "Epoch [158/200], Loss: 0.2970, Accuracy: 86.67%\n",
      "Epoch [159/200], Loss: 0.2937, Accuracy: 86.67%\n",
      "Epoch [160/200], Loss: 0.2996, Accuracy: 86.67%\n",
      "Epoch [161/200], Loss: 0.3004, Accuracy: 86.67%\n",
      "Epoch [162/200], Loss: 0.2857, Accuracy: 86.67%\n",
      "Epoch [163/200], Loss: 0.2889, Accuracy: 86.67%\n",
      "Epoch [164/200], Loss: 0.2829, Accuracy: 86.67%\n",
      "Epoch [165/200], Loss: 0.2890, Accuracy: 87.50%\n",
      "Epoch [166/200], Loss: 0.2795, Accuracy: 87.50%\n",
      "Epoch [167/200], Loss: 0.2919, Accuracy: 88.33%\n",
      "Epoch [168/200], Loss: 0.2885, Accuracy: 88.33%\n",
      "Epoch [169/200], Loss: 0.2813, Accuracy: 88.33%\n",
      "Epoch [170/200], Loss: 0.2794, Accuracy: 88.33%\n",
      "Epoch [171/200], Loss: 0.2815, Accuracy: 88.33%\n",
      "Epoch [172/200], Loss: 0.2699, Accuracy: 88.33%\n",
      "Epoch [173/200], Loss: 0.2772, Accuracy: 88.33%\n",
      "Epoch [174/200], Loss: 0.2758, Accuracy: 88.33%\n",
      "Epoch [175/200], Loss: 0.2729, Accuracy: 88.33%\n",
      "Epoch [176/200], Loss: 0.2793, Accuracy: 88.33%\n",
      "Epoch [177/200], Loss: 0.2792, Accuracy: 88.33%\n",
      "Epoch [178/200], Loss: 0.2703, Accuracy: 88.33%\n",
      "Epoch [179/200], Loss: 0.2645, Accuracy: 88.33%\n",
      "Epoch [180/200], Loss: 0.2629, Accuracy: 88.33%\n",
      "Epoch [181/200], Loss: 0.2702, Accuracy: 88.33%\n",
      "Epoch [182/200], Loss: 0.2620, Accuracy: 89.17%\n",
      "Epoch [183/200], Loss: 0.2586, Accuracy: 89.17%\n",
      "Epoch [184/200], Loss: 0.2612, Accuracy: 89.17%\n",
      "Epoch [185/200], Loss: 0.2535, Accuracy: 89.17%\n",
      "Epoch [186/200], Loss: 0.2564, Accuracy: 89.17%\n",
      "Epoch [187/200], Loss: 0.2555, Accuracy: 89.17%\n",
      "Epoch [188/200], Loss: 0.2567, Accuracy: 89.17%\n",
      "Epoch [189/200], Loss: 0.2508, Accuracy: 89.17%\n",
      "Epoch [190/200], Loss: 0.2450, Accuracy: 89.17%\n",
      "Epoch [191/200], Loss: 0.2512, Accuracy: 90.00%\n",
      "Epoch [192/200], Loss: 0.2514, Accuracy: 90.83%\n",
      "Epoch [193/200], Loss: 0.2411, Accuracy: 90.83%\n",
      "Epoch [194/200], Loss: 0.2425, Accuracy: 90.83%\n",
      "Epoch [195/200], Loss: 0.2455, Accuracy: 90.83%\n",
      "Epoch [196/200], Loss: 0.2445, Accuracy: 90.83%\n",
      "Epoch [197/200], Loss: 0.2361, Accuracy: 90.83%\n",
      "Epoch [198/200], Loss: 0.2344, Accuracy: 90.83%\n",
      "Epoch [199/200], Loss: 0.2328, Accuracy: 90.83%\n",
      "Epoch [200/200], Loss: 0.2287, Accuracy: 90.83%\n",
      "Test Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data  # Características (features)\n",
    "y = iris.target  # Rótulos (labels)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Converter para tensores do PyTorch\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar DataLoader para treino\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Criar DataLoader para teste\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Definir o modelo\n",
    "input_size = X_train.shape[1]  # 4 características de entrada\n",
    "hidden_size = 16\n",
    "output_size = len(set(y))  # 3 classes (Setosa, Versicolor, Virginica)\n",
    "dataset_samples = [list(range(10))]  # Exemplo simples, substitua conforme necessário\n",
    "threshold = 0.5\n",
    "\n",
    "model = AdvancedMLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dataset_samples=dataset_samples, threshold=threshold)\n",
    "\n",
    "# Definir a função de perda e o otimizador\n",
    "criterion = nn.CrossEntropyLoss()  # Para classificação\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento do modelo\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, reinforcement_score = model(inputs)\n",
    "        \n",
    "        # Calcular a perda\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calcular acurácia\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_preds / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Avaliar o modelo nos dados de teste\n",
    "model.eval()\n",
    "correct_preds = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs, _ = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * correct_preds / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6381829-4097-4ea5-8bb6-5af17e023bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.0505, Accuracy: 39.39%\n",
      "Epoch [2/100], Loss: 1.6215, Accuracy: 64.23%\n",
      "Epoch [3/100], Loss: 1.2095, Accuracy: 76.13%\n",
      "Epoch [4/100], Loss: 0.8808, Accuracy: 83.51%\n",
      "Epoch [5/100], Loss: 0.6603, Accuracy: 88.03%\n",
      "Epoch [6/100], Loss: 0.5127, Accuracy: 90.61%\n",
      "Epoch [7/100], Loss: 0.4131, Accuracy: 91.65%\n",
      "Epoch [8/100], Loss: 0.3439, Accuracy: 93.18%\n",
      "Epoch [9/100], Loss: 0.2926, Accuracy: 94.36%\n",
      "Epoch [10/100], Loss: 0.2548, Accuracy: 95.06%\n",
      "Epoch [11/100], Loss: 0.2242, Accuracy: 95.27%\n",
      "Epoch [12/100], Loss: 0.2002, Accuracy: 95.69%\n",
      "Epoch [13/100], Loss: 0.1800, Accuracy: 96.17%\n",
      "Epoch [14/100], Loss: 0.1631, Accuracy: 97.15%\n",
      "Epoch [15/100], Loss: 0.1489, Accuracy: 97.56%\n",
      "Epoch [16/100], Loss: 0.1370, Accuracy: 97.63%\n",
      "Epoch [17/100], Loss: 0.1264, Accuracy: 98.05%\n",
      "Epoch [18/100], Loss: 0.1174, Accuracy: 98.12%\n",
      "Epoch [19/100], Loss: 0.1089, Accuracy: 98.26%\n",
      "Epoch [20/100], Loss: 0.1018, Accuracy: 98.40%\n",
      "Epoch [21/100], Loss: 0.0955, Accuracy: 98.47%\n",
      "Epoch [22/100], Loss: 0.0893, Accuracy: 98.54%\n",
      "Epoch [23/100], Loss: 0.0843, Accuracy: 98.82%\n",
      "Epoch [24/100], Loss: 0.0793, Accuracy: 98.89%\n",
      "Epoch [25/100], Loss: 0.0749, Accuracy: 98.96%\n",
      "Epoch [26/100], Loss: 0.0710, Accuracy: 98.96%\n",
      "Epoch [27/100], Loss: 0.0669, Accuracy: 99.10%\n",
      "Epoch [28/100], Loss: 0.0635, Accuracy: 99.23%\n",
      "Epoch [29/100], Loss: 0.0604, Accuracy: 99.30%\n",
      "Epoch [30/100], Loss: 0.0573, Accuracy: 99.30%\n",
      "Epoch [31/100], Loss: 0.0547, Accuracy: 99.30%\n",
      "Epoch [32/100], Loss: 0.0523, Accuracy: 99.30%\n",
      "Epoch [33/100], Loss: 0.0494, Accuracy: 99.30%\n",
      "Epoch [34/100], Loss: 0.0473, Accuracy: 99.30%\n",
      "Epoch [35/100], Loss: 0.0453, Accuracy: 99.30%\n",
      "Epoch [36/100], Loss: 0.0432, Accuracy: 99.51%\n",
      "Epoch [37/100], Loss: 0.0410, Accuracy: 99.44%\n",
      "Epoch [38/100], Loss: 0.0392, Accuracy: 99.51%\n",
      "Epoch [39/100], Loss: 0.0373, Accuracy: 99.51%\n",
      "Epoch [40/100], Loss: 0.0355, Accuracy: 99.58%\n",
      "Epoch [41/100], Loss: 0.0337, Accuracy: 99.58%\n",
      "Epoch [42/100], Loss: 0.0325, Accuracy: 99.58%\n",
      "Epoch [43/100], Loss: 0.0308, Accuracy: 99.65%\n",
      "Epoch [44/100], Loss: 0.0296, Accuracy: 99.72%\n",
      "Epoch [45/100], Loss: 0.0282, Accuracy: 99.72%\n",
      "Epoch [46/100], Loss: 0.0270, Accuracy: 99.79%\n",
      "Epoch [47/100], Loss: 0.0261, Accuracy: 99.79%\n",
      "Epoch [48/100], Loss: 0.0248, Accuracy: 99.79%\n",
      "Epoch [49/100], Loss: 0.0239, Accuracy: 99.79%\n",
      "Epoch [50/100], Loss: 0.0228, Accuracy: 99.79%\n",
      "Epoch [51/100], Loss: 0.0217, Accuracy: 99.86%\n",
      "Epoch [52/100], Loss: 0.0207, Accuracy: 99.86%\n",
      "Epoch [53/100], Loss: 0.0200, Accuracy: 99.86%\n",
      "Epoch [54/100], Loss: 0.0190, Accuracy: 99.86%\n",
      "Epoch [55/100], Loss: 0.0182, Accuracy: 99.86%\n",
      "Epoch [56/100], Loss: 0.0175, Accuracy: 99.86%\n",
      "Epoch [57/100], Loss: 0.0168, Accuracy: 99.86%\n",
      "Epoch [58/100], Loss: 0.0160, Accuracy: 99.86%\n",
      "Epoch [59/100], Loss: 0.0154, Accuracy: 99.86%\n",
      "Epoch [60/100], Loss: 0.0147, Accuracy: 99.86%\n",
      "Epoch [61/100], Loss: 0.0141, Accuracy: 99.86%\n",
      "Epoch [62/100], Loss: 0.0135, Accuracy: 99.93%\n",
      "Epoch [63/100], Loss: 0.0130, Accuracy: 99.93%\n",
      "Epoch [64/100], Loss: 0.0125, Accuracy: 99.86%\n",
      "Epoch [65/100], Loss: 0.0121, Accuracy: 100.00%\n",
      "Epoch [66/100], Loss: 0.0116, Accuracy: 100.00%\n",
      "Epoch [67/100], Loss: 0.0110, Accuracy: 100.00%\n",
      "Epoch [68/100], Loss: 0.0107, Accuracy: 100.00%\n",
      "Epoch [69/100], Loss: 0.0102, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.0099, Accuracy: 100.00%\n",
      "Epoch [71/100], Loss: 0.0095, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.0092, Accuracy: 100.00%\n",
      "Epoch [73/100], Loss: 0.0087, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.0085, Accuracy: 100.00%\n",
      "Epoch [75/100], Loss: 0.0081, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.0079, Accuracy: 100.00%\n",
      "Epoch [77/100], Loss: 0.0075, Accuracy: 100.00%\n",
      "Epoch [78/100], Loss: 0.0073, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.0071, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.0068, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0065, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0064, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0061, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.0059, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0057, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0055, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.0053, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0052, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0050, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.0049, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.0047, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.0045, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0043, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0042, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.0041, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.0039, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.0038, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.0037, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.0036, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.0034, Accuracy: 100.00%\n",
      "Test Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset digits\n",
    "digits = load_digits()\n",
    "X = digits.data  # Características (features)\n",
    "y = digits.target  # Rótulos (labels)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Converter para tensores do PyTorch\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar DataLoader para treino\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Criar DataLoader para teste\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Definir o modelo\n",
    "input_size = X_train.shape[1]  # 4 características de entrada\n",
    "hidden_size = 16\n",
    "output_size = len(set(y))  # 3 classes (Setosa, Versicolor, Virginica)\n",
    "dataset_samples = [list(range(10))]  # Exemplo simples, substitua conforme necessário\n",
    "threshold = 0.5\n",
    "\n",
    "model = AdvancedMLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dataset_samples=dataset_samples, threshold=threshold)\n",
    "\n",
    "# Definir a função de perda e o otimizador\n",
    "criterion = nn.CrossEntropyLoss()  # Para classificação\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento do modelo\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, reinforcement_score = model(inputs)\n",
    "        \n",
    "        # Calcular a perda\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calcular acurácia\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_preds / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Avaliar o modelo nos dados de teste\n",
    "model.eval()\n",
    "correct_preds = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs, _ = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * correct_preds / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cf48e-2426-4fa6-83b6-d96f211ea5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
