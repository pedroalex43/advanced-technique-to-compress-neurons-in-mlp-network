{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4afa0-4cf4-441c-9a6a-7679d6d623d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872346f-99b1-4977-899a-ae189447e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorNeuron(nn.Module): \n",
    "    def __init__(self, input_size, dataset_samples):\n",
    "        super(SensorNeuron, self).__init__() \n",
    "        self.input_size = input_size \n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.sensory_db = self.initialize_sensory_neurons()\n",
    "        self.consolidation_counter = [0] * input_size \n",
    "        # Consolidation counters\n",
    "    def initialize_sensory_neurons(self): \n",
    "        # Initialize with weights randomly derived from dataset entries\n",
    "        sensory_neurons = [] \n",
    "        for _ in range(self.input_size):\n",
    "            random_sample = random.choice(self.dataset_samples)\n",
    "            sensory_neurons.append(random.choice(random_sample)) \n",
    "        return torch.tensor(sensory_neurons).float() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        reinforcement_score = 0\n",
    "        for i in range(len(self.sensory_db)):\n",
    "            # Check if the feature passes through the neuron using torch.isclose \n",
    "            if torch.all(torch.isclose(x[:, i], torch.tensor([self.sensory_db[i]] * x.size(0)), atol=1e-5)):\n",
    "                self.consolidation_counter[i] += 1 \n",
    "                # Increment consolidation counter \n",
    "                reinforcement_score += 1 \n",
    "            else:\n",
    "                self.consolidation_counter[i] -= 1 \n",
    "                # Decrement counter if doesn't match \n",
    "                # Replace the characteristic if the counter becomes negative \n",
    "            if self.consolidation_counter[i] < 0:\n",
    "                random_sample = random.choice(self.dataset_samples)\n",
    "                self.sensory_db[i] = random.choice(random_sample) \n",
    "                self.consolidation_counter[i] = 0\n",
    "                \n",
    "        return x, reinforcement_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1319f96-2ffc-4a78-bc5c-2303d0d493b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dataset_samples, dropout_rate=0.5):\n",
    "        super(AdvancedMLP, self).__init__()\n",
    "        self.sensor_neuron = SensorNeuron(input_size=input_size, dataset_samples=dataset_samples) \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size) \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, reinforcement_score = self.sensor_neuron(x) \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x) \n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbac0ee-3136-4b7d-927c-8c3e47479213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset Iris\n",
    "iris = load_iris() \n",
    "x_iris = iris.data\n",
    "y_iris = iris.target\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "x_treino_iris, x_teste_iris, y_treino_iris, y_teste_iris = train_test_split(x_iris, y_iris, test_size=0.3, random_state=42) \n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler() \n",
    "x_treino_iris = scaler.fit_transform(x_treino_iris)\n",
    "x_teste_iris = scaler.transform(x_teste_iris) \n",
    "# Converter para tensores do PyTorch \n",
    "x_treino_iris = torch.tensor(x_treino_iris, dtype=torch.float32)\n",
    "y_treino_iris = torch.tensor(y_treino_iris, dtype=torch.long) \n",
    "x_teste_iris = torch.tensor(x_teste_iris, dtype=torch.float32)\n",
    "y_teste_iris = torch.tensor(y_teste_iris, dtype=torch.long) \n",
    "# Parâmetros do modelo \n",
    "input_size = x_iris.shape[1] \n",
    "hidden_size = 50 \n",
    "output_size = len(np.unique(y_iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52748e0-cb9e-4a0d-bc8f-5499fe3576dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset Iris \n",
    "iris = load_iris() \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "# Dividir o dataset em treino e teste\n",
    "x_treino_iris, x_teste_iris, y_treino_iris, y_teste_iris = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "# Converter para tensores \n",
    "x_treino_iris = torch.tensor(x_treino_iris, dtype=torch.float32)\n",
    "x_teste_iris = torch.tensor(x_teste_iris, dtype=torch.float32)\n",
    "y_treino_iris = torch.tensor(y_treino_iris, dtype=torch.long)\n",
    "y_teste_iris = torch.tensor(y_teste_iris, dtype=torch.long) \n",
    "# Definir hiperparâmetros \n",
    "input_size = X.shape[1] \n",
    "hidden_size = 10 \n",
    "output_size = len(set(y)) \n",
    "# Criar e treinar o modelo\n",
    "model = AdvancedMLP(input_size, hidden_size, output_size, dataset_samples=x_treino_iris.tolist()) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "# Treinamento \n",
    "num_epochs = 1000 \n",
    "for epoch in range(num_epochs): \n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_treino_iris) \n",
    "    loss = criterion(outputs, y_treino_iris) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19e749-3057-4baf-a863-c6816153d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_teste_iris)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_teste_iris).sum().item() / y_teste_iris.size(0) \n",
    "    print(f'Acurácia no conjunto de teste: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d9643-b449-49a0-b95b-1e177117c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset Iris \n",
    "digits = load_digits() \n",
    "X = digits.data \n",
    "y = digits.target \n",
    "# Dividir o dataset em treino e teste\n",
    "x_treino_digits, x_teste_digits, y_treino_digits, y_teste_digits = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "# Converter para tensores \n",
    "x_treino_digits = torch.tensor(x_treino_digits, dtype=torch.float32)\n",
    "x_teste_digits = torch.tensor(x_teste_digits, dtype=torch.float32)\n",
    "y_treino_digits = torch.tensor(y_treino_digits, dtype=torch.long)\n",
    "y_teste_digits = torch.tensor(y_teste_digits, dtype=torch.long) \n",
    "# Definir hiperparâmetros \n",
    "input_size = X.shape[1] \n",
    "hidden_size = 10 \n",
    "output_size = len(set(y)) \n",
    "# Criar e treinar o modelo\n",
    "model = AdvancedMLP(input_size, hidden_size, output_size, dataset_samples=x_treino_digits.tolist()) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "# Treinamento \n",
    "num_epochs = 1000 \n",
    "for epoch in range(num_epochs): \n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_treino_digits) \n",
    "    loss = criterion(outputs, y_treino_digits) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567e19a-104e-492d-909c-d6afe4902a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_teste_digits)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_teste_digits).sum().item() / y_teste_digits.size(0) \n",
    "    print(f'Acurácia no conjunto de teste: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5fb48-c725-4545-96c6-33477f548cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import random \n",
    "# Carregar o conjunto de dados de dígitos \n",
    "digits = load_digits()\n",
    "X = digits.images\n",
    "y = digits.target \n",
    "# Redimensionar as imagens para [n_samples, 1, 8, 8] X = X[:, np.newaxis, :, :] \n",
    "# Dividir o conjunto de dados em treino e teste \n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "# Converter para tensores \n",
    "x_treino = torch.tensor(x_treino, dtype=torch.float32)\n",
    "x_teste = torch.tensor(x_teste, dtype=torch.float32)\n",
    "y_treino = torch.tensor(y_treino, dtype=torch.long) \n",
    "y_teste = torch.tensor(y_teste, dtype=torch.long) \n",
    "# Criar o Dataset e DataLoader\n",
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images \n",
    "        self.labels = labels \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "    \n",
    "train_dataset = DigitsDataset(x_treino, y_treino)\n",
    "test_dataset = DigitsDataset(x_teste, y_teste) \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601de185-164a-4657-a5e7-3736b65d73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura da CNN \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dataset_samples):\n",
    "        super(CNN, self).__init__() \n",
    "        self.sensor_neuron = SensorNeuron(input_size=64, dataset_samples=dataset_samples) \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, reinforcement_score = self.sensor_neuron(x) \n",
    "        x = self.pool(F.relu(self.conv1(x))) \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4) \n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.fc2(x) \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a834a-82d7-4aa8-82b6-64a4fa35c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o modelo, a função de perda e o otimizador \n",
    "model = CNN(dataset_samples=x_treino.tolist()) \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Treinar o modelo \n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    total_loss = 0 \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() \n",
    "    avg_loss = total_loss / len(train_loader) \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad030fb5-2688-4915-b251-dc1ca0d704e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
