{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74a1571-6697-40d5-8687-125a3c60fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1387c0-fbb2-449a-8691-b3502fdf0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query_weights = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_weights = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_weights = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.query_weights(x)\n",
    "        K = self.key_weights(x)\n",
    "        V = self.value_weights(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        out = self.fc_out(attention_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc48c65-0800-4e6d-9b23-52345059e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorNeuron(nn.Module):\n",
    "    def __init__(self, input_size, dataset_samples, threshold=0.5, num_heads=4):\n",
    "        super(SensorNeuron, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.threshold = threshold\n",
    "        self.sensory_db = self.initialize_sensory_neurons()\n",
    "        self.synaptic_weights = torch.ones(input_size)\n",
    "        self.consolidation_counter = [0] * input_size\n",
    "        self.attention_layer = SelfAttention(input_size, num_heads)\n",
    "        \n",
    "    def generate_geometric_patterns(self, num_patterns, pattern_size):\n",
    "        patterns = []\n",
    "        for _ in range(num_patterns):\n",
    "            pattern = np.zeros((pattern_size, pattern_size))\n",
    "            # Criar um padrão geométrico simples, como uma linha diagonal\n",
    "            for i in range(pattern_size):\n",
    "                pattern[i, i] = 1\n",
    "            patterns.append(pattern.flatten())  # Flatten para torná-los vetores\n",
    "        return np.array(patterns)\n",
    "\n",
    "    def initialize_positions(self):\n",
    "        positions = {} \n",
    "        index = 0 \n",
    "        for x in range(self.grid_size[0]):\n",
    "            for y in range(self.grid_size[1]):\n",
    "                positions[index] = (x, y) \n",
    "                index += 1\n",
    "        return positions\n",
    "\n",
    "    def initialize_sensory_neurons(self):\n",
    "        sensory_neurons = []\n",
    "        for _ in range(self.input_size):\n",
    "            random_sample = random.choice(self.dataset_samples)  # Pega um padrão aleatório\n",
    "            random_piece = np.random.choice(random_sample, self.input_size)  # Pega pedaços aleatórios\n",
    "            sensory_neurons.append(random_piece)\n",
    "    \n",
    "        # Converte a lista para um único array numpy para melhorar o desempenho\n",
    "        sensory_neurons = np.array(sensory_neurons)  \n",
    "        # Converte para tensor e retorna\n",
    "        return torch.tensor(sensory_neurons).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()  # Garantir tipo correto de entrada\n",
    "        reinforcement_score = 0  # Inicia o score de reforço\n",
    "    \n",
    "        # Garantir que a base sensorial tenha o mesmo número de entradas no batch\n",
    "        sensory_db_batch = self.sensory_db.unsqueeze(0).repeat(x.size(0), 1, 1)  # Repete a base sensorial para cada entrada no batch\n",
    "    \n",
    "        # Ordenar os índices dos neurônios de acordo com a consolidação (priorizando os reforçados) \n",
    "        sorted_indices = sorted(range(self.input_size), key=lambda i: self.consolidation_counter[i], reverse=True)\n",
    "    \n",
    "        for i in sorted_indices:\n",
    "            # Processa os neurônios reforçados primeiro \n",
    "            # Aqui estamos fazendo a comparação correta entre x e sensory_db_batch para o batch\n",
    "            diff = torch.abs(x - sensory_db_batch[:, i, :])  # Ajuste para a operação de diferença correta\n",
    "            signal_strong = diff < self.threshold  # Verifica se a maioria dos sinais é forte (True)\n",
    "        \n",
    "            if signal_strong.all():\n",
    "                # Se todos os valores no lote são menores que o limiar\n",
    "                self.consolidation_counter[i] += 1\n",
    "                reinforcement_score += 1  # Sinal forte encontrado, reforça a informação\n",
    "                self.synaptic_weights[i] += 0.1  # Fortalece a conexão (LTP)\n",
    "                # Enviar sinais para neurônios vizinhos\n",
    "                self.send_signals(i, direction=\"right\") \n",
    "                self.send_signals(i, direction=\"left\") \n",
    "                self.send_signals(i, direction=\"up\") \n",
    "                self.send_signals(i, direction=\"down\")\n",
    "            else:\n",
    "                self.consolidation_counter[i] -= 1  # Sinal fraco, reduz a consolidação\n",
    "                self.synaptic_weights[i] -= 0.1  # Enfraquece a conexão (LTD)\n",
    "                # Se a conexão se tornar negativa, restabelece o valor mínimo\n",
    "            if self.synaptic_weights[i] < 0:\n",
    "                self.synaptic_weights[i] = 0  # Garante que o peso não seja negativo\n",
    "\n",
    "            if self.consolidation_counter[i] < 0:\n",
    "                sensory_db_batch = self.update_sensory_neuron(sensory_db_batch, self.dataset_samples, i)  # Atualiza com dados aleatórios \n",
    "    \n",
    "        return x, reinforcement_score\n",
    "\n",
    "\n",
    "    def update_sensory_neuron(self, sensory_db, dataset_samples, index):\n",
    "        # Garantir que o índice esteja dentro dos limites válidos\n",
    "        index = index % sensory_db.size(0)  # Use módulo para garantir que o índice fique dentro do limite\n",
    "    \n",
    "        random_sample = random.choice(dataset_samples)\n",
    "        new_value = random.choice(random_sample)  # Valor novo para substituir\n",
    "        sensory_db_batch = torch.tensor(np.array(sensory_db)).float()\n",
    "        #sensory_db[index] = new_value  # Substituindo o valor na base sensorial\n",
    "        return sensory_db_batch #sensory_db\n",
    "\n",
    "    def send_signals(self, neuron_index, direction=\"right\"):\n",
    "        # Direções possíveis: \"right\", \"left\", \"up\", \"down\" \n",
    "        # Verifica a posição do neurônio e envia sinais aos vizinhos (ajuste no banco sensorial ou pesos)\n",
    "\n",
    "        # Exemplo de posições 2D (assumindo que você tem uma estrutura 2D de neurônios, como uma matriz/grid):\n",
    "        x, y = self.position[neuron_index]  # A posição do neurônio (supondo que você tenha um dicionário de posições)\n",
    "        new_weight = 0.05  # O quanto o sinal pode modificar o peso sináptico (ajuste livre)\n",
    "\n",
    "        # Enviar sinais para a direita\n",
    "        if direction == \"right\" and y < self.grid_size[1] - 1:\n",
    "            # Verifica se o vizinho à direita existe\n",
    "            neighbor_index = self.get_neuron_index(x, y + 1)  # Pega o índice do vizinho à direita\n",
    "            self.synaptic_weights[neighbor_index] += new_weight  # Modifica o peso sináptico\n",
    "\n",
    "        # Enviar sinais para a esquerda\n",
    "        elif direction == \"left\" and y > 0: \n",
    "            # Verifica se o vizinho à esquerda existe\n",
    "            neighbor_index = self.get_neuron_index(x, y - 1)  # Pega o índice do vizinho à esquerda\n",
    "            self.synaptic_weights[neighbor_index] += new_weight  # Modifica o peso sináptico\n",
    "\n",
    "        # Enviar sinais para cima\n",
    "        elif direction == \"up\" and x > 0:\n",
    "            # Verifica se o vizinho acima existe\n",
    "            neighbor_index = self.get_neuron_index(x - 1, y)  # Pega o índice do vizinho acima\n",
    "            self.synaptic_weights[neighbor_index] += new_weight  # Modifica o peso sináptico\n",
    "\n",
    "        # Enviar sinais para baixo\n",
    "        elif direction == \"down\" and x < self.grid_size[0] - 1:\n",
    "            # Verifica se o vizinho abaixo existe\n",
    "            neighbor_index = self.get_neuron_index(x + 1, y)  # Pega o índice do vizinho abaixo\n",
    "            self.synaptic_weights[neighbor_index] += new_weight  # Modifica o peso sináptico\n",
    "\n",
    "    \n",
    "    def reinforce_database(self): \n",
    "        # Caso o sinal seja encontrado mais de uma vez, a informação é consolidada no banco de dados permanente \n",
    "        for i in range(self.input_size):\n",
    "            if self.consolidation_counter[i] > 5:\n",
    "                # Exemplo de limiar de consolidação \n",
    "                self.sensory_db[i] = self.sensory_db[i]  # Reforço da informação \n",
    "            elif self.consolidation_counter[i] < -3:\n",
    "                self.sensory_db = self.update_sensory_neuron(self.sensory_db, self.dataset_samples, i)  # Atualiza os neurônios com base no contador de consolidação \n",
    "\n",
    "    def consolidate_after_training(self):\n",
    "        sensory_db_tensor = torch.tensor(self.sensory_db).unsqueeze(0) \n",
    "        attention_output, attention_weights = self.attention_layer(sensory_db_tensor) \n",
    "        attention_output = attention_output.squeeze(0) \n",
    "        for i in range(self.input_size):\n",
    "            if self.consolidation_counter[i] > 0: \n",
    "                self.synaptic_weights[i] += 0.1 * attention_weights[0, i].item()\n",
    "            elif self.consolidation_counter[i] < 0:\n",
    "                self.synaptic_weights[i] -= 0.1 * attention_weights[0, i].item() \n",
    "            if self.synaptic_weights[i] < 0:\n",
    "                self.synaptic_weights[i] = 0\n",
    "\n",
    "    # Novo método para resetar os contadores de reforço\n",
    "    def reset_reinforcement_scores(self):\n",
    "        self.consolidation_counter = [0] * self.input_size  # Reseta os contadores de consolidação para zero.\n",
    "        self.synaptic_weights = torch.ones(self.input_size)  # Reseta os pesos sinápticos para 1\n",
    "        print(\"Contadores de reforço e pesos sinápticos resetados.\")\n",
    "\n",
    "    def verify_geometric_pattern(self, input_pattern, tolerance=0.1):\n",
    "        correct = 0\n",
    "        for pattern in self.sensory_db:\n",
    "            if np.allclose(pattern, input_pattern, atol=tolerance):\n",
    "                correct += 1\n",
    "        accuracy = correct / len(self.sensory_db)\n",
    "        return accuracy\n",
    "\n",
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dataset_samples, threshold=0.5):\n",
    "        super(AdvancedMLP, self).__init__()\n",
    "        self.sensor_neuron = SensorNeuron(input_size=input_size, dataset_samples=dataset_samples, threshold=threshold)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x, reinforcement_score = self.sensor_neuron(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.log_softmax(x, dim=1), reinforcement_score\n",
    "\n",
    "    def train_and_consolidate(self, train_data, train_labels):\n",
    "        # Fase de Treinamento Diurno \n",
    "        for data, label in zip(train_data, train_labels):\n",
    "            self.forward(data) \n",
    "            # Fase de Consolidação Noturna \n",
    "            self.sensor_neuron.consolidate_after_training() \n",
    "            self.sensor_neuron.reset_reinforcement_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320fad4a-438c-4757-a8cf-522aaba856fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.2590, Accuracy: 18.09%\n",
      "Epoch [2/50], Loss: 2.0723, Accuracy: 39.87%\n",
      "Epoch [3/50], Loss: 1.8645, Accuracy: 51.64%\n",
      "Epoch [4/50], Loss: 1.6292, Accuracy: 61.93%\n",
      "Epoch [5/50], Loss: 1.3825, Accuracy: 71.19%\n",
      "Epoch [6/50], Loss: 1.1620, Accuracy: 77.87%\n",
      "Epoch [7/50], Loss: 0.9642, Accuracy: 83.23%\n",
      "Epoch [8/50], Loss: 0.7998, Accuracy: 87.20%\n",
      "Epoch [9/50], Loss: 0.6684, Accuracy: 89.42%\n",
      "Epoch [10/50], Loss: 0.5688, Accuracy: 90.47%\n",
      "Epoch [11/50], Loss: 0.4910, Accuracy: 91.51%\n",
      "Epoch [12/50], Loss: 0.4285, Accuracy: 92.62%\n",
      "Epoch [13/50], Loss: 0.3779, Accuracy: 93.74%\n",
      "Epoch [14/50], Loss: 0.3378, Accuracy: 94.50%\n",
      "Epoch [15/50], Loss: 0.3054, Accuracy: 94.99%\n",
      "Epoch [16/50], Loss: 0.2781, Accuracy: 95.69%\n",
      "Epoch [17/50], Loss: 0.2532, Accuracy: 96.03%\n",
      "Epoch [18/50], Loss: 0.2379, Accuracy: 96.03%\n",
      "Epoch [19/50], Loss: 0.2180, Accuracy: 96.17%\n",
      "Epoch [20/50], Loss: 0.2033, Accuracy: 96.59%\n",
      "Epoch [21/50], Loss: 0.1927, Accuracy: 96.80%\n",
      "Epoch [22/50], Loss: 0.1835, Accuracy: 96.87%\n",
      "Epoch [23/50], Loss: 0.1681, Accuracy: 96.94%\n",
      "Epoch [24/50], Loss: 0.1611, Accuracy: 97.15%\n",
      "Epoch [25/50], Loss: 0.1523, Accuracy: 97.49%\n",
      "Epoch [26/50], Loss: 0.1415, Accuracy: 97.63%\n",
      "Epoch [27/50], Loss: 0.1386, Accuracy: 97.77%\n",
      "Epoch [28/50], Loss: 0.1302, Accuracy: 97.91%\n",
      "Epoch [29/50], Loss: 0.1209, Accuracy: 98.12%\n",
      "Epoch [30/50], Loss: 0.1163, Accuracy: 98.26%\n",
      "Epoch [31/50], Loss: 0.1124, Accuracy: 97.91%\n",
      "Epoch [32/50], Loss: 0.1065, Accuracy: 98.40%\n",
      "Epoch [33/50], Loss: 0.1022, Accuracy: 98.47%\n",
      "Epoch [34/50], Loss: 0.0988, Accuracy: 98.68%\n",
      "Epoch [35/50], Loss: 0.0935, Accuracy: 98.82%\n",
      "Epoch [36/50], Loss: 0.0906, Accuracy: 98.89%\n",
      "Epoch [37/50], Loss: 0.0869, Accuracy: 98.96%\n",
      "Epoch [38/50], Loss: 0.0840, Accuracy: 98.96%\n",
      "Epoch [39/50], Loss: 0.0802, Accuracy: 99.03%\n",
      "Epoch [40/50], Loss: 0.0773, Accuracy: 99.10%\n",
      "Epoch [41/50], Loss: 0.0749, Accuracy: 99.10%\n",
      "Epoch [42/50], Loss: 0.0714, Accuracy: 99.10%\n",
      "Epoch [43/50], Loss: 0.0701, Accuracy: 99.10%\n",
      "Epoch [44/50], Loss: 0.0676, Accuracy: 99.16%\n",
      "Epoch [45/50], Loss: 0.0649, Accuracy: 99.23%\n",
      "Epoch [46/50], Loss: 0.0617, Accuracy: 99.23%\n",
      "Epoch [47/50], Loss: 0.0593, Accuracy: 99.23%\n",
      "Epoch [48/50], Loss: 0.0590, Accuracy: 99.30%\n",
      "Epoch [49/50], Loss: 0.0561, Accuracy: 99.30%\n",
      "Epoch [50/50], Loss: 0.0541, Accuracy: 99.37%\n",
      "Test Accuracy: 96.39%\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset digits\n",
    "digits = load_digits()\n",
    "X = digits.data  # Características (features)\n",
    "y = digits.target  # Rótulos (labels)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Converter para tensores do PyTorch\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar DataLoader para treino\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Criar DataLoader para teste\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definir o modelo\n",
    "input_size = X_train.shape[1]  # 4 características de entrada\n",
    "hidden_size = 16\n",
    "output_size = len(set(y))  # 3 classes (Setosa, Versicolor, Virginica)\n",
    "dataset_samples = [list(range(10))]  # Exemplo simples, substitua conforme necessário\n",
    "threshold = 0.5\n",
    "\n",
    "model = AdvancedMLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dataset_samples=dataset_samples, threshold=threshold)\n",
    "\n",
    "# Definir a função de perda e o otimizador\n",
    "criterion = nn.CrossEntropyLoss()  # Para classificação\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento do modelo\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, reinforcement_score = model(inputs)\n",
    "        \n",
    "        # Calcular a perda\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calcular acurácia\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_preds / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Avaliar o modelo nos dados de teste\n",
    "model.eval()\n",
    "correct_preds = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs, _ = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * correct_preds / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cf4de-d9f6-4fc1-bccc-26ec667f442e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
